{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stack Preparation (data-prep)\n",
    "\n",
    "This tutorial will walk you through the workflow of the [VegMapper](https://github.com/NaiaraSPinto/VegMapper) repo. At the end of this tutorial, you will create multi-band geotiffs that can be used for the identification and classification of specific agroforestry systems, such as palm-oil plantations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get credentials ##\n",
    "This repo makes use of several third-party services which will require credentials. These can be obtained using the following links. Please note that approval of an account may take several days in some cases.\n",
    "\n",
    "1) [NASA Earthdata](https://urs.earthdata.nasa.gov/users/new)\n",
    "\n",
    "2) [JAXA](https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm)\n",
    "\n",
    "3) [AWS S3/EC2](https://portal.aws.amazon.com/billing/signup#/start) or [Google Cloud Storage/GCS](https://cloud.google.com/storage/), if using cloud storage is desired\n",
    "\n",
    "4) [Google Earth Engine](https://earthengine.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-time authentication for Google Earth Engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=HzJFR3bSpLGBCiq99AX965BQyLUMw2qMz6-H1TYuAyk&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=HzJFR3bSpLGBCiq99AX965BQyLUMw2qMz6-H1TYuAyk&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: xxxxxx\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Set up data-prep conda environment ##\n",
    "\n",
    "To create the **data-prep** environment and install required packages, run these commands in your terminal:\n",
    "\n",
    "```\n",
    "% cd data-prep\n",
    "% conda env create -f data-prep-env.yml\n",
    "% conda activate data-prep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **setup.py** to verify environment and permissions of scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALOS-2/alos2_download_mosaic.py\n",
      "ALOS-2/alos2_proc.py\n",
      "Landsat/gee_export_landsat_ndvi.py\n",
      "MODIS/gee_export_modis_tc.py\n",
      "Sentinel/s1_build_vrt.py\n",
      "Sentinel/s1_metadata_summary.py\n",
      "Sentinel/s1_proc.py\n",
      "Sentinel/s1_remove_edges.py\n",
      "Sentinel/s1_submit_hyp3_jobs.py\n",
      "Stacks/build_stacks.py\n",
      "Stacks/build_condensed_stacks.py\n",
      "Utils/calc_vrt_stats.py\n",
      "Utils/prep_tiles.py\n",
      "Utils/remove_edges.py\n"
     ]
    }
   ],
   "source": [
    "%run setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set up project directory ##\n",
    "\n",
    "This repo will make use of one consistent project directory, referred from here as proj_dir. The name of proj_dir is up to you, but all subfolders and completed tiles will be generated automatically. The completed stacks as well as any intermediate products will be stored in proj_dir. At the moment, AWS S3, GCS, and local storage systems are supported. Some extra setup may be required, depending on your storage system of choice:\n",
    "\n",
    "* To set up AWS Command Line Interface (CLI) configurations and credentials (required if your proj_dir is S3):\n",
    "\n",
    "    ```\n",
    "    (data-prep) % aws configure\n",
    "    ```\n",
    "\n",
    "    where you will be asked to enter your **aws_access_key_id** and **aws_secret_access_key**.\n",
    "\n",
    "\n",
    "* To set up Google Cloud gsutil tool (required if your proj_dir is GCS):\n",
    "\n",
    "    ```\n",
    "    (data-prep) % gsutil config\n",
    "    ```\n",
    "\n",
    "    Then you will be prompted to sign in using your Google credentials. \n",
    "    \n",
    "    \n",
    "* If using local storage, create a new folder at the desired location in your filesystem. Note that using local storage is not necessarily advised as the stacks generated will be large files, and some steps may be quicker in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Prepare UTM tiles for Area of Interest (AOI) ##\n",
    "\n",
    "To create the stacks, a universal tiling system is required to ensure all data sources are aligned to the same grid. In the following section, we will generate a geoJSON file that contains the tiles to be used by all of the data processing scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep_tiles.py ###\n",
    "\n",
    "prep.tiles.py will create the tile geoJSON. It takes three required arguments: aoi_name, aoi_shp, and tile_size. aoi_name is used to name the output geojson. aoi_shp should point to a shapefile or geoJSON of the area of interest. geoJSON files for many subnational administrative boundaries can be found here: (insert link) tile_size is the desired size of each tile, in meters.\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % python prep_tiles.py [-h] aoi_name aoi_shp tile_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our area of interest is the Ucayali region of Peru, highlighted in blue:\n",
    "\n",
    "![ucayali_region](img/ucayali_region.png)\n",
    "\n",
    "Create tiles for our AOI with a tile size of 150x150 km:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles for Aoi/ucayali/ucayali: aoi/ucayali/ucayali_tiles.geojson\n",
      "14 out of 20 tiles intersecting Aoi/ucayali/ucayali\n"
     ]
    }
   ],
   "source": [
    "%run Utils/prep_tiles.py AOI/ucayali/ucayali AOI/ucayali/ucayali_boundary.geojson 150000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output tiles:\n",
    "\n",
    "![ucayali_tiles](img/ucayali_tiles.png)\n",
    "\n",
    "Note that some tiles do not intersect the region. These will be masked out and will not be used for the final stacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Prepare Sentinel-1 Tiles ##\n",
    "\n",
    "The first piece of the data stack is Sentinel-1 tiles. In the following section, we search for granules within our AOI, process them using the ASF HyP3 API, and calculate statistics for the granules, saving the results as .tif files either locally or in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for Sentinel-1 granules on [ASF Vertex](https://search.asf.alaska.edu/#/) ##\n",
    "\n",
    "1. Sign in using your Earthdata credentials. If you haven't used ASF Vertex before, you will need to agree their terms in order to use their HyP3 processing.\n",
    "\n",
    "2. Use the following \"Additional Filters\" when searching for granules within your AOI:\n",
    "\n",
    "    * File Type: L1 Detected High-Res Dual-Pol (GRD-HD)\n",
    "    * Beam Mode: IW\n",
    "    * Polarization: VV+VH\n",
    "\n",
    "    ![vertex_search_filters](img/vertex_search_filters.png)\n",
    "\n",
    "3. Add the selected granules into the download queue:\n",
    "\n",
    "    ![vertex_add_queue](img/vertex_add_queue.png)\n",
    "\n",
    "4. Download metadata files. Download at least one csv or geojson file, which will be used for submitting HyP3 jobs.\n",
    "\n",
    "    ![vertex_download_metadata](img/vertex_download_metadata.png)\n",
    "\n",
    "5. Clear the selected granules in the downloads. Do not download these GRD-HD products as we will submit HyP3 jobs to apply radiometric terrain correction (RTC) to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit HyP3 RTC jobs ##\n",
    "\n",
    "For the initial processing of the Sentinel-1 granules, we make use of ASF's HyP3 API. Information about the specifics of this processing can be found in the [HyP3 documentation](https://hyp3-docs.asf.alaska.edu/).\n",
    "\n",
    "### s1_submit_hyp3_jobs.py ###\n",
    "\n",
    "s1_submit_hyp3_jobs.py will submit the granules chosen in the previous step to the HyP3 API for processing. The results can either be saved locally or uploaded to cloud storage, such as AWS S3 or Google Cloud Storage (GCS). It takes two arguments: proj_dir, and csv/geojson. proj_dir should point to the location where results will be copied to. Supported proj_dirs are:\n",
    "* AWS S3: s3//your_bucket/some_prefix\n",
    "* GCS: gs://your_bucket/some_prefix\n",
    "* Local storage: your_local_path\n",
    "\n",
    "The processed granules will be saved in the following directory structure:\n",
    "```\n",
    "        proj_dir\n",
    "           └──sentinel_1\n",
    "               └──year\n",
    "                   └──path_frame\n",
    "                       └──processed_granules\n",
    "```\n",
    "csv/geojson should point to the location of the metadata files containing a list of granules to be submitted for processing downloaded in the previous step.\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* Since ASF HyP3 stores the processed granules in their AWS S3 buckets, the data transfer will be much faster if you set up your S3 bucket to host these data. That is, using **s3://your_bucket/some_prefix** for the proj_dir option.\n",
    "\n",
    "\n",
    "    \n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % python s1_submit_hyp3_jobs.py [-h] proj_dir csv/geojson\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit Sentinel-1 granules from the Ucayali region in the year 2017 for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Earthdata Username: xxxxxx\n",
      "Enter Earthdata Password: ········\n",
      "\n",
      "Your remaining quota for HyP3 jobs: 250 granules.\n",
      "\n",
      "You will be submitting the following granules for HyP3 RTC processing:\n",
      "    2017_171_617 - 18 granules\n",
      "    2017_171_622 - 18 granules\n",
      "    2017_25_621 - 20 granules\n",
      "    2017_25_626 - 16 granules\n",
      "    2017_25_631 - 16 granules\n",
      "    2017_98_617 - 28 granules\n",
      "    2017_98_622 - 28 granules\n",
      "    2017_98_627 - 28 granules\n",
      "\n",
      "Enter 'Y' to confirm you would like to submit these granules, or 'N' if you have already submitted the granules and want to copy the processed granules to your proj_dir: N\n",
      "\n",
      "2017_171_617: downloading processed granules to ./sentinel_1\n",
      "2017_171_617: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_171_622: downloading processed granules to ./sentinel_1\n",
      "2017_171_622: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_25_621: downloading processed granules to ./sentinel_1\n",
      "2017_25_621: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_25_626: downloading processed granules to ./sentinel_1\n",
      "2017_25_626: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_25_631: downloading processed granules to ./sentinel_1\n",
      "2017_25_631: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_98_617: downloading processed granules to ./sentinel_1\n",
      "2017_98_617: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_98_622: downloading processed granules to ./sentinel_1\n",
      "2017_98_622: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017_98_627: downloading processed granules to ./sentinel_1\n",
      "2017_98_627: There was an error when downloaing processed granules from ASF S3 bucket to .. Continuing to the next granule ...\n",
      "\n",
      "Done with everything.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 188, in main\n",
      "    download_granules(proj_dir, year, path_frame, granule_sources)\n",
      "  File \"/Users/Remy/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_submit_hyp3_jobs.py\", line 109, in download_granules\n",
      "    raise Exception('\\nJobs already expired and cannot be copied.')\n",
      "Exception: \n",
      "Jobs already expired and cannot be copied.\n"
     ]
    }
   ],
   "source": [
    "%run Sentinel/s1_submit_hyp3_jobs.py Sentinel/granules/ucayali/ucayali_sentinel_granules_2017.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 Processing ##\n",
    "\n",
    "The final processing step involves calculating the temporal mean for the Sentinel-1 granules and removing left/right (cross-track) edge pixels where border noise is prominent. \n",
    "\n",
    "### s1_proc.py ###\n",
    "\n",
    "s1_proc.py handles this final processing step. It achieves this using helper scripts in the Sentinel directory: **s1_build_vrt.py** and **calc_vrt_stats.py** for the temporal mean, and **remove_edges.py** for removing edge pixels. It takes 5 arguments: path_frame, m1, and m2 (all optional), srcpath, and year (both required). If path_frame is specified, only granules matching path_frame will be processed; otherwise, all path_frames under srcpath/year will be processed. If m1 and m2 are specified, only granules with acquisition month >= m1 and <= m2 will be processed. srcpath should point to where processed granules are stored (see previous section for valid paths). year is the year of granules to be processed.\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* The processing will be slow if srcpath is on AWS S3 or GCS because it requires heavy network I/O between the cloud and your local machine. If srcpath is on AWS S3, it is strongly recommended that you run the processing on AWS EC2.\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % python s1_proc.py [-h] [--pf path_frame] [--m1 m1] [--m2 m2] srcpath year\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all granules stored locally for the year 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Sentinel-1 RTC data in ./sentinel_1/2017 ...\n",
      "\n",
      "Processing 2017_171_622 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: sentinel_1/2017/171_622/*.zip: No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Remy/opt/anaconda3/envs/data-prep/bin/s1_build_vrt.py\", line 115, in <module>\n",
      "    main()\n",
      "  File \"/Users/Remy/opt/anaconda3/envs/data-prep/bin/s1_build_vrt.py\", line 70, in main\n",
      "    zip_list = subprocess.check_output(ls_cmd, shell=True).decode(sys.stdout.encoding).splitlines()\n",
      "  File \"/Users/Remy/opt/anaconda3/envs/data-prep/lib/python3.9/subprocess.py\", line 424, in check_output\n",
      "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "  File \"/Users/Remy/opt/anaconda3/envs/data-prep/lib/python3.9/subprocess.py\", line 528, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command 'ls sentinel_1/2017/171_622/*.zip' returned non-zero exit status 1.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 's1_build_vrt.py ./sentinel_1 2017_171_622 VV --m1 1 --m2 12' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_proc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_proc.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nProcessing Sentinel-1 RTC data in {proj_dir}/sentinel_1/{args.year} ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0ms1_proc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nDONE processing Sentinel-1 RTC data.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Cal Poly/HDD/cal poly/classes/deep gis/palm oil/VegMapper/data-prep/Sentinel/s1_proc.py\u001b[0m in \u001b[0;36ms1_proc\u001b[0;34m(storage, proj_dir, year, m1, m2, path_frame)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'VV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Build VRT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf's1_build_vrt.py {proj_dir}/sentinel_1 {year}_{path_frame} {layer} --m1 {m1} --m2 {m2}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Calculate temporal mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mvrtpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{proj_dir}/sentinel_1/{year}/{path_frame}/{year}_{path_frame}_{layer}.vrt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/data-prep/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 's1_build_vrt.py ./sentinel_1 2017_171_622 VV --m1 1 --m2 12' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%run Sentinel/s1_proc.py . 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Prepare ALOS-2 tiles ##\n",
    "\n",
    "## Download ALOS/ALOS-2 Mosaic ##\n",
    "\n",
    "### alos2_download_mosaic.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % alos2_download_mosaic.py [-h] aoi year dst\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Download ALOS/ALOS-2 Mosaic data from JAXA website.\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **aoi**\n",
    "\n",
    "      shp/geojson of area of interest (AOI)\n",
    "\n",
    "  **year**\n",
    "\n",
    "      Year\n",
    "\n",
    "  **dst**\n",
    "\n",
    "      Destination location (s3:// or gs:// or local paths). Downloaded data will be stored under dst/year/tarfiles/\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* Downloading ALOS/ALOS-2 Mosaic data requires a JAXA account, which can be registered from: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALOS-2 Processing ##\n",
    "\n",
    "### alos2_proc.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % alos2_proc.py [-h] proj_dir aoi year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Process ALOS-2 tiles by appyling an Enhanced Lee Filter (why? is that all it does?)\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **proj_dir**\n",
    "\n",
    "      project directory (s3:// or gs:// or local dir). ALOS/ALOS-2 mosaic data (.tar.gz) will be stored under\n",
    "      proj_dir/alos2_mosaic/year/tarfiles/\n",
    "\n",
    "  **aoi**\n",
    "\n",
    "      shp/geojson of area of interest (AOI)\n",
    "\n",
    "  **year**\n",
    "\n",
    "      year\n",
    "\n",
    "### Notes ###\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Prepare Landsat Tiles ##\n",
    "\n",
    "## Export Landsat NDVI ##\n",
    "\n",
    "### gee_export_landsat_ndvi.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % gee_export_landsat_ndvi.py [-h] sitename tiles res year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Submit GEE processing for Landsat NDVI. \n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **sitename**\n",
    "\n",
    "      sitename \n",
    "\n",
    "  **tiles**\n",
    "\n",
    "      shp/geojson file containing tiles onto which output raster will be resampled\n",
    "\n",
    "  **res**\n",
    "\n",
    "      Resolution\n",
    "      \n",
    "  **year**\n",
    "\n",
    "      Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Prepare MODIS Tree Cover Tiles ##\n",
    "\n",
    "## Export MODIS TC ##\n",
    "\n",
    "### gee_export_modis_tc.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % gee_export_modis_tc.py [-h] sitename tiles res year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Submit GEE processing for MODIS tree cover\n",
    "\n",
    "### Positional Arguments ###\n",
    "\n",
    "  **sitename**\n",
    "\n",
    "      sitename \n",
    "\n",
    "  **tiles**\n",
    "\n",
    "      shp/geojson file containing tiles onto which output raster will be resampled\n",
    "\n",
    "  **res**\n",
    "\n",
    "      Resolution\n",
    "      \n",
    "  **year**\n",
    "\n",
    "      Year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Build Stacks ##\n",
    "\n",
    "### build_stacks.py ###\n",
    "\n",
    "### Usage ###\n",
    "\n",
    "```\n",
    "(data-prep) % build_stacks.py [-h] [--sitename sitename] proj_dir tiles year\n",
    "```\n",
    "\n",
    "### Description ###\n",
    "\n",
    "Build 8 band stacks that include (C-VV / C-VH / C-INC / L-HH / L-HV / L-INC / NDVI / TC)\n",
    "   \n",
    "### Positional Arguments ###\n",
    "\n",
    "  **proj_dir**\n",
    "   \n",
    "      project directory (s3:// or gs:// or local dirs)\n",
    "  \n",
    "  **tiles**\n",
    "  \n",
    "      shp/geojson file that contains tiles for the output stacks\n",
    "      \n",
    "  **year** \n",
    "  \n",
    "      Year\n",
    "      \n",
    "###  Optional Arguements ###\n",
    "      \n",
    "  **sitename**\n",
    "  \n",
    "      sitename. If sitename not specified, proj_dir basename is used at sitename. \n",
    "  \n",
    "### Notes ###\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
