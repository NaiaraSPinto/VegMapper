{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaiaraSPinto/VegMapper/blob/devel-calval/calval/prepare_train_val_ref_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c515862",
      "metadata": {
        "id": "3c515862"
      },
      "source": [
        "# Creating a single training/validation/test set from multiple Collect Earth projects\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c59e929",
      "metadata": {
        "id": "0c59e929"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "* [Overview](#overview)\n",
        "* [Set-up](#setup)\n",
        "* [Sample preparation](#sample-prep)\n",
        "    * [Read-in, reshape, and recode](#reshape-recode) \n",
        "    * [Simplify the classes](#simplify)\n",
        "    * [Calculate sample agreement](#agreement)\n",
        "* [Split the dataset](#split)\n",
        "    * [Combine and convert to spatial](#combine)\n",
        "* [Key to variable names](#key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af718f4a",
      "metadata": {
        "id": "af718f4a"
      },
      "source": [
        "## Overview <a class=\"anchor\" id=\"overview\"></a>\n",
        "This notebook demonstrates how several Collect Earth Online projects can be:\n",
        "\n",
        "1. Recoded and reshaped into a structure more tractable for modeling; \n",
        "2. Merged into a single dataset that provide a single label for each sample point and an estimate of label uncertainty;\n",
        "3. Split into training, validation, and test (or map reference) samples. \n",
        "\n",
        "The data used in this demonstration are the results of three Collect Earth Online projects that were captured over the Department of Ucayali, Peru. Each project represents the efforts of an individual (or group of individuals working in the same project) to label 1350 points, classifying each into 1 of 4 classes: not oil palm; young oil palm; mature oil palm;  unsure. The datasets preserve all the information from these projects, although user email addresses were anonymized."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e56ab0",
      "metadata": {
        "id": "34e56ab0"
      },
      "source": [
        "## Sample preparation <a class=\"anchor\" id=\"sample-prep\"></a>\n",
        "Load packages with messages suppressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0968b28e",
      "metadata": {
        "scrolled": false,
        "id": "0968b28e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "from datetime import datetime as dt\n",
        "from pylab import rcParams\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## mount your Google Drive to access files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IErjaX_Mlbtj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff43b2f4-2b3f-49fe-e987-7dcfb8bfc59f"
      },
      "id": "IErjaX_Mlbtj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Google Drive folder and csv names"
      ],
      "metadata": {
        "id": "dKStwdXkSIIY"
      },
      "id": "dKStwdXkSIIY"
    },
    {
      "cell_type": "code",
      "source": [
        "googleDriveFolder = 'projects/vegmapper'\n",
        "csv1 = \"ceo-oilpalm_group1.csv\"\n",
        "csv2 = \"ceo-oilpalm_group2.csv\"\n",
        "csv3 = \"ceo-oilpalm_group3.csv\""
      ],
      "metadata": {
        "id": "tTlHshUMlrl9"
      },
      "id": "tTlHshUMlrl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set random seed for splitting data"
      ],
      "metadata": {
        "id": "oMZyoonuSNGr"
      },
      "id": "oMZyoonuSNGr"
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 999"
      ],
      "metadata": {
        "id": "fQecH6ywSOmA"
      },
      "id": "fQecH6ywSOmA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e9c30ad5",
      "metadata": {
        "id": "e9c30ad5"
      },
      "source": [
        "### Read-in, reshape, and recode classes <a class=\"anchor\" id=\"reshape-recode\"></a>\n",
        "The first step was to combine the three datasets into a single dataset, with the columns from each of the three CEO projects, and to recode the four classes into a single column with values 0 (not oil palm),  1 (young oil palm), 2 (mature oil palm), 3 (unsure). At this step, we end up with 3 columns, 1 per completed CEO project: `cl1` = samples from project 1, `cl2` = samples from project 2, `cl3` = samples from project 3. Each column contains the recoded classes (note the renaming of the columns is done in the next code chunk). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdef757d",
      "metadata": {
        "id": "cdef757d"
      },
      "outputs": [],
      "source": [
        "## need to import these survey files first (to Files on left)\n",
        "fs = [csv1, csv2, csv3]\n",
        "\n",
        "# read datasets into list, keep the most important columns, and do some renaming\n",
        "\n",
        "def process_csv(csv_path):\n",
        "  csv_data = pd.read_csv('/content/drive/MyDrive/' + googleDriveFolder + '/' + \n",
        "              csv_path, index_col=False)\n",
        "   \n",
        "  column_list = [\"plotid\", \"pl_cluster\", \"email\", \"center_lon\", \"center_lat\"] \n",
        "  column_list_oil = csv_data.filter(regex='Oil').columns.values.tolist()\n",
        "\n",
        "  csv_data = csv_data[column_list + column_list_oil]\n",
        "  csv_data = csv_data.rename(columns={\"center_lon\": \"x\", \"center_lat\": \"y\",\n",
        "                                       \"pl_cluster\": \"clust\", \"plotid\": \"id\"})\n",
        "\n",
        "  csv_data = csv_data.rename(columns=lambda x: re.sub('Oil Palm|\\\\?\\\\:| ','',x))\n",
        "\n",
        "  csv_data['class'] = csv_data.apply(\n",
        "      lambda row: 1 if row['Young'] == 100 else -9999, axis = 1\n",
        "  )\n",
        "  csv_data['class'] = csv_data.apply(\n",
        "      lambda row: 2 if \n",
        "       (row['Mature'] == 100 and row['class'] == -9999) else row['class'],\n",
        "       axis = 1\n",
        "  )\n",
        "  csv_data['class'] = csv_data.apply(\n",
        "      lambda row: 0 \n",
        "      if (row['Not'] == 100 and row['class'] == -9999) else row['class'],\n",
        "      axis = 1\n",
        "  )\n",
        "  csv_data['class'] = csv_data.apply(\n",
        "      lambda row: 3 \n",
        "      if (row['NotSure'] == 100 and row['class'] == -9999) else row['class'],\n",
        "      axis = 1\n",
        "  )\n",
        "\n",
        "  csv_data['class'] = csv_data['class'].replace(-9999, np.NaN)\n",
        "  return(csv_data)\n",
        "\n",
        "def select_columns(df):\n",
        "  df = df[['id', 'clust', 'class']]\n",
        "  return(df)\n",
        "\n",
        "\n",
        "# Map method returns a map object\n",
        "# so we cast it into list using list()\n",
        "data_list_all_columns = list(map(process_csv, fs))\n",
        "data_list = list(map(select_columns, data_list_all_columns))\n",
        "\n",
        "#print(data_list_all_columns[0])\n",
        "#print(data_list[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine into single dataset, contains just the plot id, the stratum (clust), \n",
        "# and the recoded classification columns from each input\n",
        "\n",
        "\n",
        "sample_combined_1 = pd.merge(data_list[0], data_list[1], how='left', \n",
        "                             on=['id', 'clust'], suffixes =(None, '_2'))\n",
        "sample_combined = pd.merge(sample_combined_1, data_list[2], how='left', \n",
        "                           on=['id', 'clust'], suffixes =('_1', '_3'))\n",
        "\n",
        "sample_combined = sample_combined.rename(\n",
        "    columns={\"class_1\": \"cl1\", \"class_2\": \"cl2\", \"class_3\": \"cl3\"}\n",
        ")\n",
        "#nms[grep(\"class\", nms)] <- paste0(\"cl\", 1:length(grep(\"class\", nms)))\n",
        "#names(sample_combined) = nms\n",
        "print(sample_combined)"
      ],
      "metadata": {
        "id": "kVEN9wty1jan"
      },
      "id": "kVEN9wty1jan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8dbbf0f2",
      "metadata": {
        "id": "8dbbf0f2"
      },
      "source": [
        "### Simplify the classes <a class=\"anchor\" id=\"simplify\"></a>\n",
        "\n",
        "In this step, a single classification is created by finding the modal class for each sample point across the 3 groups' results. This creates a new `class` column, which provides the class from the majority opinion. \n",
        "\n",
        "We repeat this same step again after first collapsing, within each of the `cl1:cl3` columns, the two oil palm classes into a single *oil palm* class with value = 1--*not oil palm* remains 0, and *unsure* remains 3. The modal function was re-run to create a new consensus class, called `class2`. We recommend that `class2` be used for modelling, while `class` may be useful for understanding error patterns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e983f7a4",
      "metadata": {
        "id": "e983f7a4"
      },
      "outputs": [],
      "source": [
        "# reshape and assign class as most frequently occurring value\n",
        "\n",
        "samples_red = sample_combined.copy()\n",
        "\n",
        "## create new \"cl_rpl\" columns that change values of 2 to 1\n",
        "samples_red['cl1_rpl'] = samples_red['cl1'].copy()\n",
        "samples_red.loc[samples_red.cl1 == 2, 'cl1_rpl'] = 1\n",
        "samples_red['cl2_rpl'] = samples_red['cl2'].copy()\n",
        "samples_red.loc[samples_red.cl2 == 2, 'cl2_rpl'] = 1\n",
        "samples_red['cl3_rpl'] = samples_red['cl3'].copy()\n",
        "samples_red.loc[samples_red.cl3 == 2, 'cl3_rpl'] = 1\n",
        "\n",
        "\n",
        "## take mode on original cl columns\n",
        "samples_red['class'] = samples_red[['cl1', 'cl2', 'cl3']]\\\n",
        "    .mode(axis='columns', numeric_only=True).iloc[:, 0]\n",
        "\n",
        "## take mode on new columns\n",
        "samples_red['class2'] = samples_red[['cl1_rpl', 'cl2_rpl', 'cl3_rpl']]\\\n",
        "    .mode(axis='columns', numeric_only=True).iloc[:, 0]\n",
        "\n",
        "#print(samples_red)\n",
        "\n",
        "timestamp = dt.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
        "\n",
        "with open('/content/drive/My Drive/' + googleDriveFolder + \\\n",
        "          '/test_samples_red_' + timestamp + '.csv', 'w') as f:\n",
        "  samples_red.to_csv(f, float_format='{:f}'.format, encoding='utf-8', \n",
        "                     index = False)\n",
        "\n",
        "print(\"file exported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94baf77b",
      "metadata": {
        "id": "94baf77b"
      },
      "source": [
        "### Calculate sample agreement <a class=\"anchor\" id=\"agreement\"></a>\n",
        "The next step was to calculate some agreement metrics across the three groups' samples. The primary approach was to calculate the proportion of labelling teams that selected the modal class. Since there were just three teams in this example, values were either 0.333, 0.667, 1. This agreement was calculated across for both the original classification scheme (class: 0-3) and the simplifed scheme (), with columns `agree` and `agree2` providing the respective proportions for each observation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a845bbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a845bbb",
        "outputId": "255fc1c7-6874-400d-ca3e-fdfbc9adb4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file exported\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Python code to count the number of occurrences\n",
        "def countX(lst, x):\n",
        "    return lst.count(x)\n",
        " \n",
        "\n",
        "# function to calculate agreement\n",
        "def agree_func(val, vct):\n",
        "  vct = [x for x in vct if np.isnan(x) == False]\n",
        "  agr = countX(vct, val) / len(vct)\n",
        "  return(agr)\n",
        "\n",
        "\n",
        "samples_redf = samples_red.copy()\n",
        "\n",
        "\n",
        "samples_redf['agree'] =  samples_redf.apply(\n",
        "    lambda row: agree_func(row['class'], [row['cl1'], row['cl2'], row['cl3']]),\n",
        "    axis = 1\n",
        ")\n",
        "samples_redf['agree2'] =  samples_redf.apply(\n",
        "    lambda row: agree_func(\n",
        "        row['class2'], [row['cl1_rpl'], row['cl2_rpl'], row['cl3_rpl']]\n",
        "    ), axis = 1\n",
        ")\n",
        "#print(samples_redf)\n",
        "\n",
        "with open('/content/drive/My Drive/' +\\\n",
        "          googleDriveFolder + '/test_samples_redf_' +\\\n",
        "          timestamp + '.csv', 'w') as f:\n",
        "  samples_redf.to_csv(f, float_format='{:f}'.format, encoding='utf-8', \n",
        "                      index = False)\n",
        "\n",
        "print(\"file exported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ab33e9",
      "metadata": {
        "id": "c6ab33e9"
      },
      "source": [
        "We can then calculate the average agreement per sample to get a sense of the uncertainty in labels for each class, for all 4 classes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cbab14",
      "metadata": {
        "id": "d3cbab14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35015f6-5306-4028-abba-4429bff63e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean agreement\n",
            "class                \n",
            "0.0          0.980893\n",
            "1.0          0.574713\n",
            "2.0          0.835294\n",
            "3.0          0.690476\n"
          ]
        }
      ],
      "source": [
        "sample_stats = samples_redf.groupby(\"class\").mean() \n",
        "sample_stats = sample_stats.rename(columns={\"agree\": \"mean agreement\"})\n",
        "print(sample_stats[['mean agreement']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2e124a",
      "metadata": {
        "id": "4b2e124a"
      },
      "source": [
        "And for the reduced set of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727328dc",
      "metadata": {
        "scrolled": true,
        "id": "727328dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0135ad-a443-472f-8ebb-c27a9572ab87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        mean agreement\n",
            "class2                \n",
            "0.0           0.984658\n",
            "1.0           0.893103\n",
            "3.0           0.690476\n"
          ]
        }
      ],
      "source": [
        "sample_stats_2 = samples_redf.groupby(\"class2\").mean() \n",
        "sample_stats_2 = sample_stats_2.rename(columns={\"agree2\": \"mean agreement\"})\n",
        "print(sample_stats_2[['mean agreement']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b5b0926",
      "metadata": {
        "id": "5b5b0926"
      },
      "source": [
        "## Split the dataset <a class=\"anchor\" id=\"split\"></a>\n",
        "\n",
        "Here we split the dataset into three parts for model training (60% of the sample), validation (20%), and final assessment (the 20% set aside as the test or map reference dataset).\n",
        "\n",
        "The splits are confined to the usable sample, which is defined as samples not falling into class 3 and those with at least 2/3 observers agreeing on the class. This decision is made based on the simplified sample scheme (class2), rather than the full scheme (class). The resulting splits are denoted in a column called `usage` (this is distinct from the column `use`, which was used to filter out unusable observations). \n",
        "\n",
        "Values of \"unusable\" in the `usage` column indicate observations that were not usable because of their low agreement or uncertain class.  They are included here for completeness, and in case they help with evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee232ea",
      "metadata": {
        "id": "7ee232ea"
      },
      "outputs": [],
      "source": [
        "full_sample = samples_redf.copy()\n",
        "full_sample['use'] =  full_sample.apply(\n",
        "    lambda row: 1 if (row['agree2'] > 0.5 and row['class2'] != 3) else 0 ,\n",
        "    axis = 1\n",
        ")\n",
        "full_sample['usage'] = 'NA'\n",
        "\n",
        "# do a join to get the coordinates for each site\n",
        "# read back in one of the datasets to get sample id and coordinates \n",
        "\n",
        "xy_data = data_list_all_columns[0]\n",
        "\n",
        "\n",
        "full_sample_merged = full_sample.merge(xy_data, on = 'id', how = 'left', \n",
        "                                       suffixes=(\"\", \"_y\"))\n",
        "\n",
        "full_sample_merged = full_sample_merged[\n",
        "    ['id', 'clust', 'use', 'usage', 'class2', 'agree2', 'class', 'agree', 'cl1', \n",
        "     'cl2', 'cl3', 'x', 'y']\n",
        "]\n",
        "\n",
        "#print(full_sample_merged)\n",
        "\n",
        "filtered_sample = full_sample_merged[full_sample_merged['use'] == 1]\n",
        "\n",
        "print(filtered_sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## divide filtered data into training, val, ref\n",
        "train = filtered_sample.sample(frac = 0.6, random_state = seed)\n",
        "\n",
        "not_train = filtered_sample.drop(train.index)\n",
        "val = not_train.sample(frac = 0.5, random_state = seed)\n",
        "ref = not_train.drop(val.index)\n",
        "\n",
        "train['usage'] = 'train'\n",
        "val['usage'] = 'validate'\n",
        "ref['usage'] = 'map reference/test'\n",
        "\n",
        "# Check for duplicate in each\n",
        "print('validation check: ', \n",
        "      set(val['id']).isdisjoint(set(train['id'])),\n",
        "      set(ref['id']).isdisjoint(set(train['id'])),\n",
        "      set(val['id']).isdisjoint(set(ref['id'])))\n",
        "\n",
        "#print(train)\n",
        "#print(validate)\n",
        "#print(ref)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thGPDalcyYl0",
        "outputId": "ef0df280-b137-4357-c695-328b84a38a3a"
      },
      "id": "thGPDalcyYl0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation check:  True True True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "514a3272",
      "metadata": {
        "id": "514a3272"
      },
      "source": [
        "### Combine and export to csv <a class=\"anchor\" id=\"combine\"></a>\n",
        "\n",
        "The ineligible portion of the sample is also added back for completeness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da14865",
      "metadata": {
        "id": "3da14865"
      },
      "outputs": [],
      "source": [
        "trainvalref = pd.concat([train, val, ref])\n",
        "#print(trainvalref[['id', 'clust', 'agree', 'x', 'y']])\n",
        "\n",
        "valid_ids = trainvalref['id'].values\n",
        "#print(valid_ids)\n",
        "\n",
        "omitted_rows = full_sample[~full_sample['id'].isin(valid_ids)].copy()\n",
        "omitted_rows['usage'] = 'unusable'\n",
        "\n",
        "full_samplef = pd.concat([trainvalref, omitted_rows])\n",
        "#print(full_samplef)\n",
        "\n",
        "## write csv to drive\n",
        "\n",
        "with open('/content/drive/My Drive/' +\\\n",
        "          googleDriveFolder + '/full_samplef_' + timestamp + '.csv', 'w') as f:\n",
        "  full_samplef.to_csv(f, float_format='{:f}'.format, encoding='utf-8', \n",
        "                      index = False)\n",
        "\n",
        "print('file exported')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec11c8e",
      "metadata": {
        "id": "8ec11c8e"
      },
      "source": [
        "Here is the count distribution by class and usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de046760",
      "metadata": {
        "id": "de046760"
      },
      "outputs": [],
      "source": [
        "grouped = full_samplef.groupby(['usage','class2'], group_keys=False).count()\n",
        "grouped = grouped.rename(columns={\"id\": \"count\"})\n",
        "print(grouped[['count']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ab4ddf9",
      "metadata": {
        "id": "7ab4ddf9"
      },
      "source": [
        "And their locations on a map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00737ac",
      "metadata": {
        "id": "b00737ac"
      },
      "outputs": [],
      "source": [
        "plot_sample = full_samplef.copy()\n",
        "\n",
        "usage_dict = {'train': 1, 'validate': 2, \"map reference/test\": 3, \"unusable\": 4}\n",
        "plot_sample = plot_sample.replace({'usage':usage_dict})\n",
        "\n",
        "print(plot_sample['usage'].unique())\n",
        "\n",
        "rcParams['figure.figsize'] = 10, 10\n",
        "plot_sample.plot.scatter(x='x', y='y', c='usage', s=12, cmap='viridis')\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9401597b",
      "metadata": {
        "id": "9401597b"
      },
      "source": [
        "## Key to variable names <a class=\"anchor\" id=\"key\"></a>\n",
        "\n",
        "The variable names and their descriptions are as follows:\n",
        "\n",
        "- id = Sample ID (corrected after randomized ordering)\n",
        "- clust = Stratification cluster (0, 1, 11 [previously digitized areas])\n",
        "- use = Usability of sample: 1 = suitable for training/validation/map reference, 0 = unusable\n",
        "- usage = Categorized into training, validation, and map reference (test) splits\n",
        "- class2: Classification by consensus on simplified oil palm class\n",
        "- agree2: Agreement per observation, assessed using simplified classification scheme\n",
        "- class: Consensus classification using all four classes (not recommended for modelling)\n",
        "- agree: Agreement per observation, assessed using full classification scheme\n",
        "- cl1: Labeller 1 sample\n",
        "- cl2: Labeller 2 sample\n",
        "- cl3: Labeller 3 sample\n",
        "- geometry: X and Y coordinates as simple features"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "prepare_train_val_ref_sample_colab_PY.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}