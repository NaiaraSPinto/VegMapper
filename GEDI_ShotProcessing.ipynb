{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GEDI_ShotProcessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bronte999/VegMapper/blob/master/GEDI_ShotProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GEDI Shot Processing\n",
        "\n",
        "\n",
        "Script to process and filter shot data from  \"Footprint level canopy height and profile metrics\" dataset located at: https://gedi.umd.edu/data/products/ \n"
      ],
      "metadata": {
        "id": "HTw6Nh4-pv1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements: \n",
        "\n",
        "Access to bucket (use EC2 or other trusted entity that can read/write) (link to tutorial)"
      ],
      "metadata": {
        "id": "Pqe9Cd7IpX2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install h5py numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-65nLc_07hoe",
        "outputId": "797e1408-6450-410e-a386-2e21294e7f89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import itertools\n",
        "import statistics\n",
        "import time\n",
        "#import gsutil\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zuBk6adH_7s0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##User input\n",
        "These are global variables defined by the user of this code. Please set these before running your own experiment. \n",
        "\n",
        "`h5FilesToProcess`: Specify location of text file containing a list of the names of h5 files to process.\n",
        "\n",
        "`sourceDirectory`: Specify the directory where the h5 files are located. This must be located in a bucket (either cloud or S3) that user can read/write to in current environment. \n",
        "\n",
        "`destinationFilePrefix`: Prefix to append to individual csv files after extracting data from h5 file. For example, if processing `GEDI02_A_2020198095831_O09023_T00771_02_001_01.h5` the resulting csv file will be `PREFIX_GEDI02_A_2020198095831_O09023_T00771_02_001_01.csv`\n",
        "\n",
        "`destinationDirectory`: Directory where individual csv files and final aggregated csv file will be saved. This doesn't have to be a bucket. \n",
        "\n",
        "`outputFileName`: Name of the final aggregated csv file. This is contains all of the data from each of the individual csv files generated. "
      ],
      "metadata": {
        "id": "pk7v1lmAm9nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h5FilesToProcess = \"h5S3files.txt\"\n",
        "sourceDirectory = \"s3://servir-public/gedi/peru/\"\n",
        "destinationFilePrefix = \"filtered_\"\n",
        "destinationDirectory = \"s3://servir-public/gedi/peru/\"\n",
        "outputFileName = \"FILTERED_CSV.csv\""
      ],
      "metadata": {
        "id": "7KUtWn-rABAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Process:\n",
        "For each h5 file listed in `h5FilesToProcess`...\n",
        "\n",
        "1. Download the file\n",
        "2. Extract and filter data for each beam in the shot file\n",
        "3. Save filtered data for all beams in the shot in csv \n",
        "4. Delete downloaded file and repeat 1-3\n",
        "5. Concatenate/aggregate all shot data in one csv"
      ],
      "metadata": {
        "id": "ptZUYC2VhaX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main\n",
        "\n",
        "Sets up experiment constants and "
      ],
      "metadata": {
        "id": "IrRJBcgI75B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    csv_files = [] ## list that keeps track of all csv files generated\n",
        "\n",
        "    ##generate column names for rh vals (ranges 1-100) \n",
        "    rh_cols = []\n",
        "    for i in range(101):\n",
        "      rh_cols.append('rh'+str(i))\n",
        "\n",
        "    readH5Files(csv_files, rh_cols)\n",
        "    print(\"FINISHED\")"
      ],
      "metadata": {
        "id": "lc40_H597qYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##readH5Files\n",
        "Downloads one h5 file at a time and reads it. If the file is not an h5 file or is corrupted (can't be read) the downloaded file is deleted and the next file is downloaded to be read. Once the file is downloaded and verified to be a readable h5 file, processBeams is called. "
      ],
      "metadata": {
        "id": "PC8sQ2nQ99R1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL2qEo1L_qWY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def readH5Files(csv_files, rh_cols):\n",
        "  f = open(h5FilesToProcess, \"r\")\n",
        "\n",
        "  for h5file in f.readlines(): \n",
        "      h5file = h5file.strip()\n",
        "\n",
        "      #download the h5file from aws\n",
        "      cpS3url = \"aws s3 cp \"+ sourceDirectory+ + h5file + \" ./\"\n",
        "      \n",
        "      print(\"DOWNLOAD FILE: \" + cpS3url)\n",
        "      os.system(cpS3url)\n",
        "\n",
        "      ###READ DATA\n",
        "      try:\n",
        "          gediL2A = h5py.File(h5file, 'r')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "          os.remove(h5file)\n",
        "      else:\n",
        "        processBeams(gediL2A, h5file, csv_files, rh_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##processBeams\n",
        "Each shot file contains information for multiple beams. Data will be extracted from each beam (`extractBeamData()`), one at a time, and then combined to be filtered by `filterBeamData`. "
      ],
      "metadata": {
        "id": "TDlm9XDD_p1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processBeams(gediL2A, h5file, csv_files, rh_cols):\n",
        "    ## approx 4 beams in every file\n",
        "    beamNames = [g for g in gediL2A.keys() if g.startswith('BEAM')]\n",
        "    beam_dfs = [] #create list to hold resulting dataframes for each beam\n",
        "\n",
        "    gediL2A_objs = []\n",
        "    gediL2A.visit(gediL2A_objs.append)                                           # Retrieve list of datasets\n",
        "    gediSDS = [o for o in gediL2A_objs if isinstance(gediL2A[o], h5py.Dataset)]  # Search for relevant SDS inside data file \n",
        "\n",
        "    for highBeam in beamNames: #extract data from each beam in the shot\n",
        "      beam_df = extractBeamData(highBeam, gediL2A, rh_cols)\n",
        "      if beam_df: #if beam data extraction is successful, append resulting dataframe to list \n",
        "        beam_dfs.append(beam_df)\n",
        "    \n",
        "    if len(beam_dfs) >= 2:\n",
        "      all_beam_dfs = pd.concat(beam_dfs) #combine all beam dataframes\n",
        "      filterBeamData(all_beam_dfs, h5file, csv_files)\n",
        "    elif len(beam_dfs) == 1:\n",
        "      filterBeamData(beam_dfs, h5file, csv_files)\n",
        "    else:\n",
        "      return"
      ],
      "metadata": {
        "id": "dOLkkFj-whwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##extractBeamData\n",
        "Return selected beam features in pandas dataframe.  "
      ],
      "metadata": {
        "id": "dlhBwDF9RNy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractBeamData(highBeam, gediL2A, rh_cols):\n",
        "        gedi_beam_data = {}\n",
        "        try:\n",
        "            gedi_beam_data[\"lats\"] = gediL2A[f'{highBeam}/lat_lowestmode'][()]\n",
        "            gedi_beam_data[\"long\"] = gediL2A[f'{highBeam}/lon_lowestmode'][()]\n",
        "            gedi_beam_data[\"shotNumber\"] = gediL2A[f'{highBeam}/shot_number'][()]\n",
        "            gedi_beam_data[\"quality\"] = gediL2A[f'{highBeam}/quality_flag'][()]\n",
        "            gedi_beam_data[\"solar_elev\"] = gediL2A[f'{highBeam}/solar_elevation'][()]\n",
        "            gedi_beam_data[\"sensitivity\"] = gediL2A[f'{highBeam}/sensitivity'][()]\n",
        "            #gedi_beam_data[\"rh\"] = gediL2A[f'{highBeam}/rh'][()]\n",
        "            gedi_beam_data[\"elevLow\"] = gediL2A[f'{highBeam}/elev_lowestmode'][()]\n",
        "\n",
        "            gedi_beam_data[\"delta_time\"] = gediL2A[f'{highBeam}/delta_time'][()]\n",
        "            gedi_beam_data[\"energy_total\"] = gediL2A[f'{highBeam}/energy_total'][()]\n",
        "                \n",
        "            gedi_beam_data[\"rx_gbias\"] = gediL2A[f'{highBeam}/rx_1gaussfit/rx_gbias'][()]\n",
        "            gedi_beam_data[\"rx_gamplitude_error\"] = gediL2A[f'{highBeam}/rx_1gaussfit/rx_gamplitude_error'][()]\n",
        "                \n",
        "            gedi_beam_data[\"rx1_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a1/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx2_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a2/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx3_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a3/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx4_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a4/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx5_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a5/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx6_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a6/energy_sm'][()]\n",
        "             \n",
        "            gedi_beam_data[\"rx1_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a1/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx2_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a2/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx3_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a3/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx4_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a4/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx5_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a5/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx6_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a6/lastmodeenergy'][()]\n",
        "                \n",
        "            gedi_beam_data[\"elev1_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a1'][()]\n",
        "            gedi_beam_data[\"elev2_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a2'][()]\n",
        "            gedi_beam_data[\"elev3_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a3'][()]\n",
        "            gedi_beam_data[\"elev4_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a4'][()]\n",
        "            gedi_beam_data[\"elev5_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a5'][()]\n",
        "            gedi_beam_data[\"elev6_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a6'][()]\n",
        "                \n",
        "            gedi_beam_data[\"elev1_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a1'][()]\n",
        "            gedi_beam_data[\"elev2_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a2'][()]\n",
        "            gedi_beam_data[\"elev3_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a3'][()]\n",
        "            gedi_beam_data[\"elev4_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a4'][()]\n",
        "            gedi_beam_data[\"elev5_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a5'][()]\n",
        "            gedi_beam_data[\"elev6_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a6'][()]\n",
        "             \n",
        "            gedi_beam_data[\"elev1\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a1'][()]\n",
        "            gedi_beam_data[\"elev2\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a2'][()]\n",
        "            gedi_beam_data[\"elev3\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a3'][()]\n",
        "            gedi_beam_data[\"elev4\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a4'][()]\n",
        "            gedi_beam_data[\"elev5\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a5'][()]\n",
        "            gedi_beam_data[\"elev6\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a6'][()]\n",
        "            rh = gediL2A[f'{highBeam}/rh'][()]\n",
        "            gedi_beam_data[\"beam\"] = np.full(len(gedi_beam_data['shotNumber']), highBeam)\n",
        "\n",
        "            raw_df = pd.DataFrame(gedi_beam_data)\n",
        "            raw_df[rh_cols] = pd.DataFrame(rh, index= raw_df.index) #add rh values\n",
        "            raw_df = raw_df.dropna() \n",
        "        except Exception as e:\n",
        "            print(\"EXCEPTION: \", e)\n",
        "            return None\n",
        "        else:\n",
        "          return raw_df"
      ],
      "metadata": {
        "id": "bD_ksopVweMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##rangeCalculator\n",
        "This function is used to calculate the range of elevations between elevation modes. Only considers values that are within 2 standarad deviations of the mean. If none of the values are are within 2 standard deviations of the mean then a dummy value is returned to flag this row to be discarded. This method ensures that the range is not affected by outlier elevation observations.  "
      ],
      "metadata": {
        "id": "VeamecG7RseT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## returns the range between valid elevations (elevations that are within 2SD of the mean)\n",
        "def rangeCalculator(elevList, sd, mean):\n",
        "    elevList = [item for item in elevList if item <= (mean + (2*sd)) and item >= (mean - (2*sd)) ]\n",
        "    if elevList:\n",
        "        return max(elevList) - min(elevList)\n",
        "    else: \n",
        "        ##all elevations are abnormal, return -99 to indicate data from this row should not be included in analysis\n",
        "        return 99"
      ],
      "metadata": {
        "id": "K-42o85FxiUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##filterBeamData\n",
        "After extracting the data from each beam, the beam data needs to be filtered. The criteria we are filtering on is as follows:\n",
        "1. Sensitivity greater than 90%\n",
        "2. Elevation range (from rangeCalculator) is less than 2\n",
        "3. rh95 is greater than 0 (not ground point)\n",
        "\n",
        "Once this data is filtered, `saveFilteredData` is called. Data is only saved if filtering is successful, if an error occurs the process does not continue. "
      ],
      "metadata": {
        "id": "WCse8D3qSlJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filterBeamData(all_beam_dfs, h5file, csv_files):\n",
        "  try:\n",
        "    filtered =  all_beam_dfs[all_beam_dfs.sensitivity >= 0.9] #sensitivity > 90%\n",
        "          \n",
        "    #filter based on elevation\n",
        "    elevs = pd.concat([filtered.elev1, filtered.elev2, filtered.elev3, filtered.elev4, filtered.elev5, filtered.elev6])\n",
        "    filtered['elev_sd'] = elevs.std()\n",
        "    filtered['elev_mean'] = elevs.mean() #calculate mean of elevs\n",
        "    filtered['elev_range'] = filtered.apply(lambda x: rangeCalculator([x['elev1'], x['elev2'], x['elev3'], x['elev4'], x['elev5'], x['elev6']], x['elev_sd'], x['elev_mean']), axis = 1)\n",
        "          \n",
        "    GEDI_DF = filtered[abs(filtered.elev_range) < 2] #only select rows that have valid ranges\n",
        "\n",
        "    GEDI_DF = GEDI_DF[GEDI_DF.rh95 > 0] ## excludes ground points\n",
        "  except Exception as e:\n",
        "    print(\"EXCEPTION: \", e)\n",
        "    return\n",
        "  else:\n",
        "    saveFilteredData(GEDI_DF, h5file, csv_files)"
      ],
      "metadata": {
        "id": "nJpZaVEAwbPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##saveFilteredData\n",
        "Saves shot data to an individual csv file in the current directory, then moves the file to the destination directory. The name of the csv file is the name of the h5 file and the user specified prefix. The processed h5 file is then deleted from the local directory to free up space. "
      ],
      "metadata": {
        "id": "hln2Jgf2T57I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saveFilteredData(GEDI_DF, h5file, csv_files):\n",
        "  #save final filtered dataframe to a csv file in the destination directory with the correct prefix\n",
        "  csv_file = destinationFilePrefix + h5file[:-3]+\".csv\"\n",
        "  csv_files.append(csv_file)\n",
        "  print(\"CREATING CSV: \" + csv_file)\n",
        "  GEDI_DF.to_csv(csv_file)\n",
        "  cp_csv = \"aws s3 cp \" + csv_file + \" \" + destinationDirectory + csv_file\n",
        "  print(\"COPY CSV TO S3: \" + cp_csv)\n",
        "  os.system(cp_csv)\n",
        "  os.remove(csv_file)\n",
        "  try:\n",
        "    os.remove(h5file)\n",
        "  except Exception as e:\n",
        "    print(\"EXCEPTION: ,\", e)"
      ],
      "metadata": {
        "id": "PPnL_Na_wadt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entire process of downloading the raw shot data, extracting data from individual beams, filtering the sho tdata, and saving the result repeats for the next h5 file listed in the `h5FileFilesToProcess` "
      ],
      "metadata": {
        "id": "QcbVcNAWUuHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Concatenating the Results\n",
        "This step is optional. The individual csv files generated for each h5 file can be concatenated into a single csv file. This is not memory optimized and may not be feasible for larger areas of interest. "
      ],
      "metadata": {
        "id": "uZlEWTwpheFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_to_concat = []\n",
        "for f in csv_files:\n",
        "    local_filename = f.replace(destinationDirectory,'') #remove path info, string should now be of format: prefix_name.csv\n",
        "\n",
        "    cpS3url = \"aws s3 cp \" + f +  \" \" + local_filename #download csv file from destination directory, save to local\n",
        "    print(cpS3url)\n",
        "    os.system(cpS3url)\n",
        "\n",
        "    csv_df =  pd.read_csv(f) #create df from file\n",
        "    files_to_concat.append(csv_df) #append dataframe to list to concat later\n",
        "    os.remove(local_filename) #remove file when done, save space \n",
        "\n",
        "df = pd.concat(files_to_concat) #concat all the files\n",
        "df.to_csv(outputFileName, index = False) #create final dataframe"
      ],
      "metadata": {
        "id": "erA7msybDQlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}