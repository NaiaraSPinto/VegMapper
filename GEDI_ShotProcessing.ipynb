{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GEDI_ShotProcessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bronte999/VegMapper/blob/master/GEDI_ShotProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GEDI Shot Processing\n",
        "\n",
        "\n",
        "Script to process and filter shot data from  \"Footprint level canopy height and profile metrics\" dataset located at: https://gedi.umd.edu/data/products/ \n",
        "\n",
        "Only shots that fit the following criteria are kept:\n",
        "- Sensitivity > 0.9\n",
        "- Elevation lowest mode within 2 SD, range < 2 m for all algorithms\n",
        "- Exclude ground points\n"
      ],
      "metadata": {
        "id": "HTw6Nh4-pv1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements: \n",
        "\n",
        "Access to bucket (use EC2 or other trusted entity that can read/write) (link to tutorial)"
      ],
      "metadata": {
        "id": "Pqe9Cd7IpX2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install h5py numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-65nLc_07hoe",
        "outputId": "797e1408-6450-410e-a386-2e21294e7f89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import itertools\n",
        "import statistics\n",
        "import time\n",
        "#import gsutil\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zuBk6adH_7s0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h5FilesToProcess = \"h5S3files.txt\"\n",
        "sourceDirectory = \"s3://servir-public/gedi/peru/\"\n",
        "destinationFilePrefix = \"filtered_\"\n",
        "destinationDirectory = \"s3://servir-public/gedi/peru/\"\n",
        "outputFileName = \"FILTERED_CSV.csv\""
      ],
      "metadata": {
        "id": "7KUtWn-rABAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "aM9mviO_93xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## returns the range between valid elevations (elevations that are within 2SD of the mean)\n",
        "def rangeCalculator(elevList, sd, mean):\n",
        "    elevList = [item for item in elevList if item <= (mean + (2*sd)) and item >= (mean - (2*sd)) ]\n",
        "    if elevList:\n",
        "        return max(elevList) - min(elevList)\n",
        "    else: \n",
        "        ##all elevations are abnormal, return -99 to indicate data from this row should not be included in analysis\n",
        "        return 99"
      ],
      "metadata": {
        "id": "NbZFiJUb91dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing the Data\n",
        "\n",
        "description\n",
        "downloading the data\n",
        "extracting the data into the dataframe\n",
        "filter based on elevation, rh values, sensitivity"
      ],
      "metadata": {
        "id": "ptZUYC2VhaX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL2qEo1L_qWY"
      },
      "outputs": [],
      "source": [
        "## list that keeps track of all csv files generated\n",
        "csv_files = []\n",
        "\n",
        "f = open(h5FilesToProcess, \"r\")\n",
        "\n",
        "##generate column names for rh vals (ranges 1-100) \n",
        "rh_cols = []\n",
        "for i in range(101):\n",
        "  rh_cols.append('rh'+str(i))\n",
        "\n",
        "#process every h5file\n",
        "for h5file in f.readlines():\n",
        "    h5file = h5file.strip()\n",
        "\n",
        "    #download the h5file from aws\n",
        "    cpS3url = \"aws s3 cp \"+ sourceDirectory+ + h5file + \" ./\"\n",
        "    \n",
        "    print(\"DOWNLOAD FILE: \" + cpS3url)\n",
        "    os.system(cpS3url)\n",
        "\n",
        "    ###READ DATA\n",
        "    try:\n",
        "        gediL2A = h5py.File(h5file, 'r')\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "    ## approx 4 beams in every file\n",
        "    beamNames = [g for g in gediL2A.keys() if g.startswith('BEAM')]\n",
        "    beam_dfs = [] #create list to hold resulting dataframes for each beam\n",
        "\n",
        "\n",
        "    gediL2A_objs = []\n",
        "    gediL2A.visit(gediL2A_objs.append)                                           # Retrieve list of datasets\n",
        "    gediSDS = [o for o in gediL2A_objs if isinstance(gediL2A[o], h5py.Dataset)]  # Search for relevant SDS inside data file\n",
        "\n",
        "    ###SELECT DATA FROM EACH BEAM     \n",
        "    for highBeam in beamNames:\n",
        "        extractionSuccessful = True #boolean to store extraction status\n",
        "        gedi_beam_data = {}\n",
        "        try:\n",
        "            gedi_beam_data[\"lats\"] = gediL2A[f'{highBeam}/lat_lowestmode'][()]\n",
        "            gedi_beam_data[\"long\"] = gediL2A[f'{highBeam}/lon_lowestmode'][()]\n",
        "            gedi_beam_data[\"shotNumber\"] = gediL2A[f'{highBeam}/shot_number'][()]\n",
        "            gedi_beam_data[\"quality\"] = gediL2A[f'{highBeam}/quality_flag'][()]\n",
        "            gedi_beam_data[\"solar_elev\"] = gediL2A[f'{highBeam}/solar_elevation'][()]\n",
        "            gedi_beam_data[\"sensitivity\"] = gediL2A[f'{highBeam}/sensitivity'][()]\n",
        "            #gedi_beam_data[\"rh\"] = gediL2A[f'{highBeam}/rh'][()]\n",
        "            gedi_beam_data[\"elevLow\"] = gediL2A[f'{highBeam}/elev_lowestmode'][()]\n",
        "\n",
        "            gedi_beam_data[\"delta_time\"] = gediL2A[f'{highBeam}/delta_time'][()]\n",
        "            gedi_beam_data[\"energy_total\"] = gediL2A[f'{highBeam}/energy_total'][()]\n",
        "                \n",
        "            gedi_beam_data[\"rx_gbias\"] = gediL2A[f'{highBeam}/rx_1gaussfit/rx_gbias'][()]\n",
        "            gedi_beam_data[\"rx_gamplitude_error\"] = gediL2A[f'{highBeam}/rx_1gaussfit/rx_gamplitude_error'][()]\n",
        "                \n",
        "            gedi_beam_data[\"rx1_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a1/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx2_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a2/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx3_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a3/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx4_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a4/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx5_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a5/energy_sm'][()]\n",
        "            gedi_beam_data[\"rx6_energy_sm\"] = gediL2A[f'{highBeam}/rx_processing_a6/energy_sm'][()]\n",
        "             \n",
        "            gedi_beam_data[\"rx1_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a1/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx2_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a2/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx3_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a3/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx4_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a4/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx5_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a5/lastmodeenergy'][()]\n",
        "            gedi_beam_data[\"rx6_lastmodeenergy\"] = gediL2A[f'{highBeam}/rx_processing_a6/lastmodeenergy'][()]\n",
        "                \n",
        "            gedi_beam_data[\"elev1_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a1'][()]\n",
        "            gedi_beam_data[\"elev2_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a2'][()]\n",
        "            gedi_beam_data[\"elev3_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a3'][()]\n",
        "            gedi_beam_data[\"elev4_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a4'][()]\n",
        "            gedi_beam_data[\"elev5_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a5'][()]\n",
        "            gedi_beam_data[\"elev6_low_energy\"] = gediL2A[f'{highBeam}/geolocation/energy_lowestmode_a6'][()]\n",
        "                \n",
        "            gedi_beam_data[\"elev1_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a1'][()]\n",
        "            gedi_beam_data[\"elev2_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a2'][()]\n",
        "            gedi_beam_data[\"elev3_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a3'][()]\n",
        "            gedi_beam_data[\"elev4_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a4'][()]\n",
        "            gedi_beam_data[\"elev5_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a5'][()]\n",
        "            gedi_beam_data[\"elev6_num_modes\"] = gediL2A[f'{highBeam}/geolocation/num_detectedmodes_a6'][()]\n",
        "             \n",
        "            gedi_beam_data[\"elev1\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a1'][()]\n",
        "            gedi_beam_data[\"elev2\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a2'][()]\n",
        "            gedi_beam_data[\"elev3\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a3'][()]\n",
        "            gedi_beam_data[\"elev4\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a4'][()]\n",
        "            gedi_beam_data[\"elev5\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a5'][()]\n",
        "            gedi_beam_data[\"elev6\"] = gediL2A[f'{highBeam}/geolocation/elev_lowestmode_a6'][()]\n",
        "            rh = gediL2A[f'{highBeam}/rh'][()]\n",
        "            gedi_beam_data[\"beam\"] = np.full(len(gedi_beam_data['shotNumber']), highBeam)\n",
        "\n",
        "            raw_df = pd.DataFrame(gedi_beam_data)\n",
        "            raw_df[rh_cols] = pd.DataFrame(rh, index= raw_df.index)\n",
        "            raw_df = raw_df.dropna()\n",
        "            beam_dfs.append(raw_df)\n",
        "        except Exception as e:\n",
        "            print(\"EXCEPTION: \", e)\n",
        "            extractionSuccessful = False #indicates further analysis should not be done\n",
        "\n",
        "\n",
        "    if extractionSuccessful: ##if all data was able to be extracted from the beam\n",
        "        all_beam_dfs = pd.concat(beam_dfs) #combine all beam dataframes\n",
        "\n",
        "        ## BEGIN FILTERING\n",
        "        filtered =  all_beam_dfs[all_beam_dfs.sensitivity >= 0.9] #keep night shots and sensitivity > 90%\n",
        "        \n",
        "        #filter based on elevation\n",
        "        elevs = pd.concat([filtered.elev1, filtered.elev2, filtered.elev3, filtered.elev4, filtered.elev5, filtered.elev6])\n",
        "        filtered['elev_sd'] = elevs.std()\n",
        "        filtered['elev_mean'] = elevs.mean() #calculate mean of elevs\n",
        "        filtered['elev_range'] = filtered.apply(lambda x: rangeCalculator([x['elev1'], x['elev2'], x['elev3'], x['elev4'], x['elev5'], x['elev6']], x['elev_sd'], x['elev_mean']), axis = 1)\n",
        "        \n",
        "        GEDI_DF = filtered[abs(filtered.elev_range) < 2] #only select rows that have valid ranges\n",
        "\n",
        "        GEDI_DF = GEDI_DF[GEDI_DF.rh95 > 0] ## excludes ground points\n",
        "        \n",
        "        #save final filtered dataframe to a csv file in the destination directory with the correct prefix\n",
        "        csv_file = destinationFilePrefix + h5file[:-3]+\".csv\"\n",
        "        csv_files.append(csv_file)\n",
        "        print(\"CREATING CSV: \" + csv_file)\n",
        "        GEDI_DF.to_csv(csv_file)\n",
        "        cp_csv = \"aws s3 cp \" + csv_file + \" \" + destinationDirectory + csv_file\n",
        "        print(\"COPY CSV TO S3: \" + cp_csv)\n",
        "        os.system(cp_csv)\n",
        "        os.remove(csv_file)\n",
        "    try:\n",
        "        os.remove(h5file)\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Concatenating the Results"
      ],
      "metadata": {
        "id": "uZlEWTwpheFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_to_concat = []\n",
        "for f in csv_files:\n",
        "    local_filename = f.replace(destinationDirectory,'') #remove path info, string should now be of format: prefix_name.csv\n",
        "\n",
        "    cpS3url = \"aws s3 cp \" + f +  \" \" + local_filename #download csv file from destination directory, save to local\n",
        "    print(cpS3url)\n",
        "    os.system(cpS3url)\n",
        "\n",
        "    csv_df =  pd.read_csv(f) #create df from file\n",
        "    files_to_concat.append(csv_df) #append dataframe to list to concat later\n",
        "    os.remove(local_filename) #remove file when done, save space \n",
        "\n",
        "df = pd.concat(files_to_concat) #concat all the files\n",
        "df.to_csv(outputFileName, index = False) #create final dataframe"
      ],
      "metadata": {
        "id": "erA7msybDQlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}