{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPERA-RTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import asf_search as asf\n",
    "\n",
    "#aws-related imports\n",
    "from getpass import getpass\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "from vegmapper import s1\n",
    "from vegmapper import pathurl\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['figure.figsize'] = [16, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "In the cell below, make sure to provide the following inputs\n",
    "- **Define** site name.\n",
    "- **Locate** or create a project directory.\n",
    "- **Define** the path of the site boundary layer.\n",
    "  - Currently expected to be inside the project directory\n",
    "- **Define** the path to the boundary tile if needed (should be automatically created inside the project directory)\n",
    "- **Define** the start and end times of interest\n",
    "- **Provide** NASA Earth Data token file location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site name\n",
    "sitename = 'ucayali'\n",
    "\n",
    "# Project directory (local path or cloud bucket URL)\n",
    "proj_dir = './ucayali'\n",
    "\n",
    "# AOI file\n",
    "aoifile = f'{proj_dir}/ucayali_boundary.geojson'\n",
    "\n",
    "# Reference tiles\n",
    "tiles = f'{proj_dir}/{sitename}_tiles.geojson'\n",
    "\n",
    "# Start and end dates of interest\n",
    "start_date = '2024-06-01' # june-sep measure the time it take to average for a burst first for a burst and later for ucayali \n",
    "end_date = '2024-10-01'\n",
    "\n",
    "# Sentinel-1 paltform S1A or S1B, S1 for both\n",
    "platform = 'S1'\n",
    "\n",
    "# RTC directory\n",
    "rtc_dir = f'{proj_dir}/opera_rtc'\n",
    "\n",
    "# NASA Earth Data token\n",
    "ned_token = f'{proj_dir}/ned.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check project directory\n",
    "proj_dir_path = Path(proj_dir)\n",
    "# Create the directory if it doesn't exist\n",
    "if not proj_dir_path.exists():\n",
    "    print(f\"The project diretory: {proj_dir_path}, was not found. Please make sure its correct\")\n",
    "else:\n",
    "    print(f\"Current project directory: {proj_dir_path}\")\n",
    "\n",
    "# verify that the aoifile exits.\n",
    "warnings.simplefilter(\"always\", UserWarning)\n",
    "aoifile_path = Path(aoifile)\n",
    "tiles_path = Path(tiles)\n",
    "if aoifile_path.is_file() or tiles_path.is_file():\n",
    "    print(f\"Boundary file found: {aoifile_path}\")\n",
    "    print(f\"Tiles file found: {tiles_path}\")\n",
    "else:\n",
    "    # Raise a warning if the file does not exist\n",
    "    warnings.warn(\n",
    "        f\"Boundary file {aoifile_path} or {tiles_path} was not found or does not exist, \"\n",
    "        \"please make sure the file paths are correct before continuing\",\n",
    "        UserWarning\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing steps \n",
    "1. [Granule Search](#1.-Granule-Search)\n",
    "2. [Access RTC data and form temporal average](#2.-Access-RTC-data-and-form-temporal-average)\n",
    "3. [Run RTC load + temp average](#3.-Run-RTC-load-+-temp-average)\n",
    "4. [Create VRT tiles](#4.-Create-VRT-tiles)\n",
    "5. [Export tiles to an S3 bucket](#5.-Export-tiles-to-an-S3-bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AOI\n",
    "gdf_aoi = gpd.read_file(aoifile)\n",
    "gdf_aoi.plot(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 1. Granule Search\n",
    "\n",
    "ASF DAAC uses *granules* and *scenes* interchangeably to refer to a Sentinel-1 product temporally and geographically, whereas *frames* are used to refer to the geolocation only for a Sentinel-1 product. The naming convention for a Sentinel-1 granule can be found [here](https://asf.alaska.edu/data-sets/sar-data-sets/sentinel-1/sentinel-1-data-and-imagery/). Each *frame* can be uniquely identified by a pair of *path* and *frame* numbers. In this section, we will search for Sentinel-1 granules that intersect with AOI and were acquired between the start and end dates.\n",
    "\n",
    "### `s1.search_granules`\n",
    "\n",
    "```\n",
    "s1.search_granules(sitename, aoifile, start_date, end_date, skim=True, **search_opts)\n",
    "```\n",
    "\n",
    "Paremeters:\n",
    "\n",
    "|Paremeters|Description|Required|Default|\n",
    "|----|----|----|----|\n",
    "|sitename|Site name|Yes||\n",
    "|aoifile|AOI file in vector-based spatial data format (shapefile, GeoJSON, ...)|Yes||\n",
    "|start_date|Start date (YYYY-MM-DD)|Yes||\n",
    "|end_date|End date (YYYY-MM-DD)|Yes||\n",
    "|skim|Skim the search results so only the frames that just cover the AOI are retained|No|True|\n",
    "|search_opts|Search options for ASF Python module (asf_search). See [here](https://docs.asf.alaska.edu/asf_search/searching/).|No|True|\n",
    "\n",
    "Returns:\n",
    "\n",
    "|Returns|Description|\n",
    "|----|----|\n",
    "|gdf_granules|A GeoDataFrame containing all searched granules along with their detailed properties|\n",
    "|gdf_frames|A GeoDataFrame of `gdf_granules` grouped by frames.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the query has been ran before load the query instead of running it again.\n",
    "granule_query_file = f\"{proj_dir}/{sitename}_granules.shp\"\n",
    "if os.path.exists(granule_query_file):\n",
    "    print(f\"The file {granule_query_file} exists. Using existing file for current run.\")\n",
    "    print(\"- To run a new query please delete the current one in the project directory\")\n",
    "    gdf_granules = gpd.read_file(granule_query_file)\n",
    "else:\n",
    "    print(f\"The file {granule_query_file} does not exist. Running query\")\n",
    "    # Here we search for Sentinel-1 OPERA-RTC products acquired with Interferometric Wide (IW) beam mode and both VV and VH polarizations.\n",
    "    search_opts = {\n",
    "        'dataset': asf.DATASET.OPERA_S1\n",
    "    }\n",
    "    gdf_granules = s1.search_granules(sitename, aoifile, start_date, end_date, skim=True, **search_opts)\n",
    "    # export \n",
    "    gdf_granules.to_file(granule_query_file, driver=\"ESRI Shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_granules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granules found over study area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tiles\n",
    "gdf_tiles = gpd.read_file(tiles).to_crs(epsg=4326)\n",
    "\n",
    "# Plot search results\n",
    "ax = gdf_aoi.plot(figsize=(10, 10))\n",
    "gdf_granules.boundary.plot(ax=ax, color='red')\n",
    "gdf_tiles.boundary.plot(ax=ax, color='black')\n",
    "if (gdf_tiles['mask'] == 0).any():\n",
    "    ax = gdf_tiles[gdf_tiles['mask'] == 0].plot(ax=ax, color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst count overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate burst summary geodataframe and include the flightpath\n",
    "# Get RTC burst id\n",
    "gdf_granules['burst_id'] = gdf_granules['fileID'].str[:31]\n",
    "\n",
    "# Group by burst id and count number of bursts per burst_id\n",
    "burst_counts = gdf_granules.groupby('burst_id').size().reset_index(name='count')\n",
    "\n",
    "# Group by burst id and aggregate the geometries\n",
    "grouped_bursts = gdf_granules.groupby('burst_id').agg({\n",
    "    'geometry': lambda x: x.union_all(),\n",
    "    'pathNumber': lambda x: x.mode() if x.mode().size > 0 else x.iloc[0]\n",
    "})\n",
    "\n",
    "# Reset index to convert the resulting Series to DataFrame\n",
    "grouped_bursts = grouped_bursts.reset_index()\n",
    "\n",
    "# Merge the grouped geometries and counts back to fileID_grouped_counts\n",
    "burst_summary = pd.merge(burst_counts, grouped_bursts, on='burst_id', how='left')\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "burst_summary_gdf = gpd.GeoDataFrame(burst_summary, geometry='geometry')\n",
    "\n",
    "# Ensure CRS is set before exporting\n",
    "if burst_summary_gdf.crs is None:\n",
    "    burst_summary_gdf.set_crs(gdf_granules.crs, allow_override=True, inplace=True) \n",
    "\n",
    "# export \n",
    "burst_summary_gdf.to_file(f\"{proj_dir}/{sitename}_burst_summary.geojson\", driver=\"GeoJSON\")\n",
    "burst_summary_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting acquisition summary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(burst_summary_gdf.index, burst_summary_gdf['count'])\n",
    "plt.xlabel('Burst index number (grouped by burst ID)')\n",
    "plt.ylabel('Acquisitions per burst')\n",
    "plt.title('Burst acquisition summary')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get burst summary by count\n",
    "count_list = burst_summary_gdf['count'].unique()\n",
    "\n",
    "# Filter the GeoDataFrame to extract rows where count is equal to the indicated value\n",
    "def filter_count_gdf(in_df, value):\n",
    "    filtered_gdf = in_df[in_df['count'] == value]\n",
    "    \n",
    "    return filtered_gdf \n",
    "\n",
    "# extract geometries to use in plot\n",
    "burst_count_gdfs = []\n",
    "for count in count_list:\n",
    "    burst_count_gdfs.append(gpd.GeoDataFrame((filter_count_gdf(burst_summary_gdf, count)), geometry='geometry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot search results\n",
    "cmap = plt.colormaps.get_cmap('Set2')\n",
    "ax = gdf_aoi.plot(figsize=(10, 10))\n",
    "for i, count_gdf in enumerate(burst_count_gdfs):\n",
    "    count_gdf.boundary.plot(ax=ax, color=cmap(i))\n",
    "gdf_tiles.boundary.plot(ax=ax, color='black')\n",
    "if (gdf_tiles['mask'] == 0).any():\n",
    "    ax = gdf_tiles[gdf_tiles['mask'] == 0].plot(ax=ax, color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Access RTC data and form temporal average\n",
    "\n",
    "- The following cells will generate temporary credentials to access the ASF data bucket. \n",
    "- This will allow us to retrive the desired OPERA-RTC products.\n",
    "- A free Earthdata login account is required.\n",
    "-  Go to the link below and create one if you haven't done that yet:\n",
    "    -  [Create Earthdata account](https://www.earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/earthdata-login)\n",
    "-  Once your account, login to Earthdata using your credentials.\n",
    "-  Now we should generate a Bearer Token to access the data bucket following th einstructions below.\n",
    "    - [Instructions for creating an EDL Bearer Token](https://urs.earthdata.nasa.gov/documentation/for_users/user_token)\n",
    "-  Copy your Bearer Token and paste it when asked in the cell below.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request S3 credencials\n",
    "**Enter your Earthdata Login Bearer Token**\n",
    "\n",
    "The cells below will create temporary credentials to access the OPERA-RTC data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the token from a file instead of typing it\n",
    "with open(ned_token, \"r\") as file:\n",
    "    token = file.read().strip()\n",
    "print(\"Token successfully loaded from file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"OPERA_L2_RTC-S1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "    \"CredentialsEndpoint\": \"https://cumulus.asf.alaska.edu/s3credentials\",\n",
    "    \"BearerToken\": token,\n",
    "    \"Bucket\": \"asf-cumulus-prod-opera-products\",\n",
    "    \"Prefix\": prefix,\n",
    "    \"StaticPrefix\": f\"{prefix}_STATIC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get temporary download credentials\n",
    "tea_url = event[\"CredentialsEndpoint\"]\n",
    "bearer_token = event[\"BearerToken\"]\n",
    "req = urllib.request.Request(\n",
    "    url=tea_url,\n",
    "    headers={\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    ")\n",
    "with urllib.request.urlopen(req) as f:\n",
    "    creds = json.loads(f.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run RTC load + temp average \n",
    "\n",
    "In this section we will estimate the temporal mean for each available burst.\n",
    "  \n",
    "**Required inputs:**\n",
    "- List of burst id's \n",
    "- Geodataframe with granule data\n",
    "- Earthdata temporary credentials\n",
    "\n",
    "`Outputs will be stored as:`\n",
    "- opera_rtc/burstID_tmean_polarization.tif\n",
    "\n",
    "`For example:` \n",
    "- opera_rtc/OPERA_L2_RTC-S1_T025-052685-IW1_tmean_VV.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the available burstID to loop over their available data\n",
    "burst_id_list = burst_summary_gdf['burst_id'].unique().tolist()\n",
    "\n",
    "# Extract dates for file name \n",
    "s_date = start_date.replace('-', '')\n",
    "e_date = end_date.replace('-', '')\n",
    "\n",
    "# Create temporal mean for all available bursts\n",
    "s1.run_rtc_temp_mean(burst_id_list, \n",
    "                     gdf_granules, \n",
    "                     creds, \n",
    "                     event, \n",
    "                     rtc_dir,\n",
    "                     s_date,\n",
    "                     e_date,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample temporal mean\n",
    "with rasterio.open(f'{rtc_dir}/{burst_id_list[0]}_tmean_{s_date}_{e_date}_VV.tif') as dset:\n",
    "    VV1 = dset.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(VV1, vmin=0, vmax=0.5, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create VRT tiles\n",
    "In this section we will generate virtual raster tiles using the predefined reference tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First map the burst to tiles\n",
    "burst_tile_gdf = s1.map_burst2tile(tiles, burst_summary_gdf, rtc_dir)\n",
    "\n",
    "# use the table above to run the main flow\n",
    "tile_info = s1.check_tiles_exist(burst_tile_gdf, f\"{rtc_dir}/tile_vrts\", sitename, s_date, e_date)\n",
    "if tile_info != 'All tiles exist':\n",
    "    print('---> Creating tiles')\n",
    "    s1.build_opera_vrt(burst_tile_gdf, \n",
    "                       rtc_dir,\n",
    "                       sitename,\n",
    "                       s_date, \n",
    "                       e_date,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a sample tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(f'{rtc_dir}/tile_vrts/s1_tile_{sitename}_{s_date}_{e_date}_h1_v1_VV.tif') as dset:\n",
    "    VV = dset.read(1)\n",
    "VV[VV == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot VV of tile h1v1\n",
    "plt.imshow(VV, vmin=0, vmax=0.5, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RVI for each tile\n",
    "The Radar Vegetation Index (RVI) is defined as:\n",
    "\n",
    "$RVI = \\frac{4CX}{CO + CX}$\n",
    "\n",
    "where $CO$ and $CX$ are the co-polarized (VV or HH) and cross-polarized (VH or HV) radar backscatter in linear scale, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.compute_rvi_tiles(f\"{rtc_dir}/tile_vrts\", sitename, s_date, e_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that all tiles have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all tiles exist \n",
    "_ = s1.check_tiles_exist(burst_tile_gdf, f\"{rtc_dir}/tile_vrts\", sitename, s_date, e_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporal averages if not needed anymore\n",
    "# for rtc_path in glob.glob(f\"{rtc_dir}/*_tmean_*.tif\"):\n",
    "#     os.remove(rtc_path)\n",
    "#     print(f\"Deleted: {rtc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vrt mosaics \n",
    "s1.create_vrt_mosaic(f\"{rtc_dir}/tile_vrts\", sitename, s_date, e_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(f\"{rtc_dir}/tile_vrts/s1_tile_mosaic_{sitename}_{s_date}_{e_date}_VV.vrt\") as dset:\n",
    "    mosaic = dset.read(1)\n",
    "mosaic[mosaic == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot VV of tile h1v1\n",
    "plt.imshow(mosaic, vmin=0, vmax=0.5, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export tiles to an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to a bucket. \n",
    "import boto3\n",
    "# S3 bucket name\n",
    "bucket_name = \"your-s3-bucket-name\"\n",
    "\n",
    "# Local folder containing files to upload\n",
    "local_folder = \"/path/to/your/files\"\n",
    "\n",
    "# Directory name in the S3 bucket where files will be uploaded\n",
    "s3_directory = \"your-target-directory\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for root, dirs, files in os.walk(local_folder):\n",
    "    for file in files:\n",
    "        # Check if the file starts with 'tile'\n",
    "        if file.startswith(\"tile\"):\n",
    "            # Full local file path\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Create the S3 key with the specified directory\n",
    "            s3_key = f\"{s3_directory}/{file}\"\n",
    "\n",
    "            # Upload file\n",
    "            s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "            print(f\"Uploaded {file_path} to s3://{bucket_name}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents in the specified S3 directory\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=f\"{s3_directory}/\")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print(f\"Files in s3://{bucket_name}/{s3_directory}/:\")\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])  # Print the full S3 key of each object\n",
    "else:\n",
    "    print(f\"No files found in s3://{bucket_name}/{s3_directory}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vegmapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
